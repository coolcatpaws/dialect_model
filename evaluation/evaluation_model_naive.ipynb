{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81779e69-bebb-4f3e-9e5d-00bffb329032",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:22:46.558684Z",
     "iopub.status.busy": "2025-09-15T02:22:46.558529Z",
     "iopub.status.idle": "2025-09-15T02:22:48.308793Z",
     "shell.execute_reply": "2025-09-15T02:22:48.308251Z",
     "shell.execute_reply.started": "2025-09-15T02:22:46.558665Z"
    }
   },
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fa8ce2f-04f7-4f5c-a24a-22fe3e15ae9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:22:50.228438Z",
     "iopub.status.busy": "2025-09-15T02:22:50.228221Z",
     "iopub.status.idle": "2025-09-15T02:22:50.463065Z",
     "shell.execute_reply": "2025-09-15T02:22:50.462489Z",
     "shell.execute_reply.started": "2025-09-15T02:22:50.228421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? True\n",
      "Device count: 1\n",
      "Device name: NVIDIA A10G\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available?\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac229404-3d3b-43e9-a78f-f1bf4a2afc98",
   "metadata": {},
   "source": [
    "## Naive model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be07493-8659-4fda-b99c-fefedb0b2275",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:22:53.184551Z",
     "iopub.status.busy": "2025-09-15T02:22:53.184333Z",
     "iopub.status.idle": "2025-09-15T02:25:16.932172Z",
     "shell.execute_reply": "2025-09-15T02:25:16.931491Z",
     "shell.execute_reply.started": "2025-09-15T02:22:53.184536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n",
      "Copied 8 GRU tensors from pretrained checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss=0.3320 | val_acc=93.75% | 23.2s\n",
      "New best model saved at epoch 1 (val_acc=93.75%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | loss=0.2277 | val_acc=90.00% | 12.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | loss=0.2166 | val_acc=93.75% | 12.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | loss=0.1886 | val_acc=90.00% | 12.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | loss=0.1811 | val_acc=91.25% | 12.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | loss=0.1860 | val_acc=90.00% | 12.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | loss=0.1795 | val_acc=91.25% | 12.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | loss=0.1698 | val_acc=91.25% | 12.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | loss=0.1677 | val_acc=92.50% | 11.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | loss=0.1613 | val_acc=91.25% | 12.1s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ----------------- Config (EFS paths, batch, etc.) -----------------\n",
    "EFS_ROOT   = \"/mnt/custom-file-systems/efs/fs-0a84517bf3cf54d59_fsap-04bd72a3f345b82c0/dialect-modeling\"\n",
    "MODEL_PATH = f\"{EFS_ROOT}/models/model9.model\"\n",
    "TRAIN_PATH = f\"{EFS_ROOT}/data/demo/train.parquet\"\n",
    "TEST_PATH  = f\"{EFS_ROOT}/data/demo/test.parquet\"\n",
    "\n",
    "BATCH_SIZE   = 64\n",
    "EPOCHS       = 10\n",
    "LR           = 1e-3\n",
    "TARGET_SR    = 16000\n",
    "N_MELS       = 40\n",
    "N_FFT        = 400\n",
    "HOP_LENGTH   = 160\n",
    "MAX_LEN      = 200     # frames (pad / truncate to this)\n",
    "\n",
    "NUM_WORKERS  = 2       # set to 0 if you see multiprocessing/pickling issues\n",
    "PIN_MEMORY   = True\n",
    "\n",
    "# ----------------- Model -----------------\n",
    "class LanNetBinary(nn.Module):\n",
    "    def __init__(self, input_dim=40, hidden_dim=512, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim,\n",
    "                          num_layers=num_layers, batch_first=True)\n",
    "        self.linear2 = nn.Linear(hidden_dim, 192)\n",
    "        self.linear3 = nn.Linear(192, 2)   # binary output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F)\n",
    "        out, _ = self.gru(x)               # (B, T, H)\n",
    "        last = out[:, -1, :]               # (B, H)\n",
    "        x = self.linear2(last)             # (B, 192)\n",
    "        x = self.linear3(x)                # (B, 2)\n",
    "        return x\n",
    "\n",
    "# ----------------- Feature extraction (from arrays) -----------------\n",
    "def ensure_mono(y):\n",
    "    y = np.asarray(y)\n",
    "    if y.ndim == 2:\n",
    "        # (channels, samples) or (samples, channels) — make best guess\n",
    "        if y.shape[0] < y.shape[1]:  # channels likely first\n",
    "            y = y.mean(axis=0)\n",
    "        else:\n",
    "            y = y.mean(axis=1)\n",
    "    return y.astype(np.float32, copy=False)\n",
    "\n",
    "def resample_if_needed(y, sr_in, sr_out=TARGET_SR):\n",
    "    if sr_in is None or sr_in == 0:\n",
    "        sr_in = sr_out\n",
    "    if sr_in == sr_out:\n",
    "        return y, sr_out\n",
    "    return librosa.resample(y, orig_sr=sr_in, target_sr=sr_out), sr_out\n",
    "\n",
    "def fbanks_from_array(y, sr=TARGET_SR,\n",
    "                      n_mels=N_MELS, n_fft=N_FFT,\n",
    "                      hop_length=HOP_LENGTH, max_len=MAX_LEN):\n",
    "    # y must be mono, float32, sr == TARGET_SR\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr,\n",
    "                                         n_mels=n_mels,\n",
    "                                         n_fft=n_fft,\n",
    "                                         hop_length=hop_length,\n",
    "                                         power=2.0)\n",
    "    fbanks = librosa.power_to_db(mel).T   # (time, 40)\n",
    "    T = fbanks.shape[0]\n",
    "    if T < max_len:\n",
    "        fbanks = np.pad(fbanks, ((0, max_len - T), (0, 0)), mode=\"constant\")\n",
    "    else:\n",
    "        fbanks = fbanks[:max_len, :]\n",
    "    return fbanks.astype(np.float32)\n",
    "\n",
    "# ----------------- Dataset -----------------\n",
    "class FbankArrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expects df columns:\n",
    "      - 'audio': 1D (samples,) or 2D array (mix); dtype numeric\n",
    "      - optional 'sampling_rate': int; if missing, assume TARGET_SR\n",
    "      - label column: either 'label' (0/1 or 'shanghai'/other) or 'dialect'\n",
    "    \"\"\"\n",
    "    def __init__(self, df, label_col=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        if label_col is None:\n",
    "            self.label_col = \"label\" if \"label\" in self.df.columns else \"dialect\"\n",
    "        else:\n",
    "            self.label_col = label_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        y = ensure_mono(row[\"audio\"])\n",
    "        sr = row[\"sampling_rate\"] if \"sampling_rate\" in row and not pd.isna(row[\"sampling_rate\"]) else TARGET_SR\n",
    "        y, _ = resample_if_needed(y, int(sr), TARGET_SR)\n",
    "\n",
    "        x = fbanks_from_array(y, sr=TARGET_SR)          # (MAX_LEN, 40)\n",
    "\n",
    "        label = row[self.label_col]\n",
    "        if isinstance(label, str):\n",
    "            label_bin = 1 if label.lower() == \"shanghai\" else 0\n",
    "        else:\n",
    "            label_bin = int(label)\n",
    "\n",
    "        return torch.from_numpy(x), torch.tensor(label_bin, dtype=torch.long)\n",
    "\n",
    "# ----------------- Load dataframes (from EFS) -----------------\n",
    "train_df = pd.read_parquet(TRAIN_PATH)\n",
    "test_df  = pd.read_parquet(TEST_PATH)\n",
    "\n",
    "# create/normalize binary label if needed\n",
    "if \"label\" not in train_df.columns:\n",
    "    if \"dialect\" in train_df.columns:\n",
    "        train_df[\"label\"] = (train_df[\"dialect\"].str.lower() == \"shanghai\").astype(int)\n",
    "        test_df[\"label\"]  = (test_df[\"dialect\"].str.lower() == \"shanghai\").astype(int)\n",
    "    else:\n",
    "        raise ValueError(\"Expected 'label' or 'dialect' in parquet with audio arrays.\")\n",
    "\n",
    "# ----------------- Datasets & DataLoaders -----------------\n",
    "train_ds = FbankArrayDataset(train_df, label_col=\"label\")\n",
    "test_ds  = FbankArrayDataset(test_df,  label_col=\"label\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "# ----------------- Load pretrained GRU weights (from EFS) -----------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training on:\", device)\n",
    "\n",
    "binary_model = LanNetBinary().to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "orig_state = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
    "\n",
    "# Map 'layer1.GRU.*' -> 'gru.*' where shapes match\n",
    "with torch.no_grad():\n",
    "    own_state = binary_model.state_dict()\n",
    "    copied = 0\n",
    "    for k, v in orig_state.items():\n",
    "        if k.startswith(\"layer1.GRU.\"):\n",
    "            new_k = k.replace(\"layer1.GRU.\", \"gru.\")\n",
    "            if new_k in own_state and own_state[new_k].shape == v.shape:\n",
    "                own_state[new_k].copy_(v)\n",
    "                copied += 1\n",
    "    print(f\"Copied {copied} GRU tensors from pretrained checkpoint.\")\n",
    "\n",
    "# Freeze GRU initially (train head first)\n",
    "for p in binary_model.gru.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# ----------------- Optimizer / Loss -----------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, binary_model.parameters()), lr=LR)\n",
    "\n",
    "# ----------------- Eval helper -----------------\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            logits = model(xb)\n",
    "            preds  = logits.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total   += yb.numel()\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "# ----------------- Train loop ------------------\n",
    "# %% Training with checkpoint saving\n",
    "checkpoint_dir = f\"{EFS_ROOT}/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "best_acc = 0.0\n",
    "best_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "\n",
    "history = {\"loss\": [], \"val_acc\": [], \"epoch_secs\": []}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    binary_model.train()\n",
    "    t0 = time.time()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = binary_model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / max(len(train_loader), 1)\n",
    "    val_acc = evaluate(binary_model, test_loader, device)\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    history[\"loss\"].append(avg_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"epoch_secs\"].append(dt)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | loss={avg_loss:.4f} | val_acc={val_acc:.2%} | {dt:.1f}s\")\n",
    "\n",
    "    # ----------------- Save best checkpoint -----------------\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(binary_model.state_dict(), best_path)\n",
    "        print(f\"New best model saved at epoch {epoch} (val_acc={val_acc:.2%})\")\n",
    "\n",
    "\n",
    "# (Optional) Unfreeze GRU to fine-tune end-to-end with a smaller LR:\n",
    "# for p in binary_model.gru.parameters():\n",
    "#     p.requires_grad = True\n",
    "# optimizer = optim.Adam(binary_model.parameters(), lr=5e-4)\n",
    "# ...train a few more epochs...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf946a9-5c07-42b2-aab3-c03b8f7f6fcd",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9839c6-f0f3-45a7-9402-53d75740ea91",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-15T02:14:29.315985Z",
     "iopub.status.idle": "2025-09-15T02:14:29.316174Z",
     "shell.execute_reply": "2025-09-15T02:14:29.316093Z",
     "shell.execute_reply.started": "2025-09-15T02:14:29.316083Z"
    }
   },
   "outputs": [],
   "source": [
    "# %% Visualizations: curves + confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 1) Training curves\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history[\"loss\"], marker=\"o\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history[\"val_acc\"], marker=\"o\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) Confusion matrix on test set\n",
    "all_preds, all_labels = [], []\n",
    "binary_model.eval()\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        logits = binary_model(xb)\n",
    "        preds = logits.argmax(1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "labels = [\"Not-Shanghai\", \"Shanghai\"]\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=[0,1])\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=labels, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10467fe0-a39c-4c32-86c1-fe9a99ff22dd",
   "metadata": {},
   "source": [
    "## Export to HF\n",
    "\n",
    "Run after training; reads existing variables (paths, config) and exports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e013fa-816f-4dae-a1fa-64fafa386ee3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:43:33.600325Z",
     "iopub.status.busy": "2025-09-15T02:43:33.600079Z",
     "iopub.status.idle": "2025-09-15T02:43:42.511593Z",
     "shell.execute_reply": "2025-09-15T02:43:42.511002Z",
     "shell.execute_reply.started": "2025-09-15T02:43:33.600304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export complete:\n",
      "Model dir: /mnt/custom-file-systems/efs/fs-0a84517bf3cf54d59_fsap-04bd72a3f345b82c0/dialect-modeling/hf/models/shanghai-binary\n",
      "Dataset dir: /mnt/custom-file-systems/efs/fs-0a84517bf3cf54d59_fsap-04bd72a3f345b82c0/dialect-modeling/hf/datasets/shanghai-binary\n"
     ]
    }
   ],
   "source": [
    "# %% Export model + configs + dataset into HF-style folders on EFS\n",
    "import os, json, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from safetensors.torch import save_file as save_safetensors  # pip install safetensors\n",
    "\n",
    "# --------- Inputs (edit names) ---------\n",
    "MODEL_NAME = \"shanghai-binary\"\n",
    "MODEL_DIR_EFS = os.path.join(EFS_ROOT, \"hf\", \"models\", MODEL_NAME)\n",
    "DS_NAME = \"shanghai-binary\"\n",
    "DS_DIR_EFS = os.path.join(EFS_ROOT, \"hf\", \"datasets\", DS_NAME)\n",
    "CKPT_DIR = \"/mnt/custom-file-systems/efs/fs-0a84517bf3cf54d59_fsap-04bd72a3f345b82c0/dialect-modeling/checkpoints\"\n",
    "BEST_CKPT = os.path.join(CKPT_DIR, \"best_model.pth\")   # from your training loop\n",
    "TRAIN_SRC = TRAIN_PATH                                        # parquet you trained on\n",
    "TEST_SRC = TEST_PATH                                         # parquet you evaluated on\n",
    "\n",
    "os.makedirs(MODEL_DIR_EFS, exist_ok=True)\n",
    "os.makedirs(os.path.join(DS_DIR_EFS, \"data\"), exist_ok=True)\n",
    "\n",
    "# --------- 1) Save model weights in safetensors (recommended) ---------\n",
    "state_dict = torch.load(BEST_CKPT, map_location=\"cpu\")\n",
    "weights_path = os.path.join(MODEL_DIR_EFS, \"model.safetensors\")\n",
    "save_safetensors(state_dict, weights_path)\n",
    "\n",
    "# (Optionally also save PyTorch .bin)\n",
    "# torch.save(state_dict, os.path.join(MODEL_DIR_EFS, \"pytorch_model.bin\"))\n",
    "\n",
    "# --------- 2) Save model config (architecture) ---------\n",
    "config = {\n",
    "    \"_name_or_path\": MODEL_NAME,\n",
    "    \"model_type\": \"gru-audio-binary\",\n",
    "    \"input_dim\": 40,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_labels\": 2,\n",
    "    \"classifier_dims\": [192, 2],\n",
    "    \"pooling\": \"last_timestep\"\n",
    "}\n",
    "with open(os.path.join(MODEL_DIR_EFS, \"config.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "# --------- 3) Save preprocessor config (feature extraction settings) ---------\n",
    "preproc = {\n",
    "    \"feature_type\": \"fbank\",\n",
    "    \"sampling_rate\": TARGET_SR,\n",
    "    \"n_mels\": N_MELS,\n",
    "    \"n_fft\": N_FFT,\n",
    "    \"hop_length\": HOP_LENGTH,\n",
    "    \"max_len_frames\": MAX_LEN,\n",
    "    \"log_db\": True,\n",
    "    \"mono\": True\n",
    "}\n",
    "with open(os.path.join(MODEL_DIR_EFS, \"preprocessor_config.json\"), \"w\") as f:\n",
    "    json.dump(preproc, f, indent=2)\n",
    "\n",
    "# --------- 4) Save label mapping ---------\n",
    "label_map = { \"0\": \"not-shanghai\", \"1\": \"shanghai\" }\n",
    "with open(os.path.join(MODEL_DIR_EFS, \"label_mapping.json\"), \"w\") as f:\n",
    "    json.dump(label_map, f, indent=2)\n",
    "\n",
    "# --------- 5) Minimal model README (model card-style) ---------\n",
    "model_readme = f\"\"\"# {MODEL_NAME}\n",
    "\n",
    "Binary classifier: **Shanghai** vs **Not-Shanghai** (audio FBANK → GRU → MLP).\n",
    "\n",
    "## Files\n",
    "- `model.safetensors` — PyTorch weights (safetensors)\n",
    "- `config.json` — model architecture\n",
    "- `preprocessor_config.json` — audio feature extraction settings\n",
    "- `label_mapping.json` — index → label\n",
    "\n",
    "## Inference (PyTorch)\n",
    "```python\n",
    "import torch, json, numpy as np, librosa\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "\n",
    "# Load config\n",
    "import json, os\n",
    "model_dir = \"./hf/models/{MODEL_NAME}\"\n",
    "cfg = json.load(open(os.path.join(model_dir, \"config.json\")))\n",
    "pp  = json.load(open(os.path.join(model_dir, \"preprocessor_config.json\")))\n",
    "lm  = json.load(open(os.path.join(model_dir, \"label_mapping.json\")))\n",
    "\n",
    "# Define the model class you trained (LanNetBinary)\n",
    "# (Same as in your training notebook)\n",
    "class LanNetBinary(torch.nn.Module):\n",
    "    def __init__(self, input_dim=40, hidden_dim=512, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.gru = torch.nn.GRU(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, 192)\n",
    "        self.linear3 = torch.nn.Linear(192, 2)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        last = out[:, -1, :]\n",
    "        x = self.linear2(last)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "# Load weights\n",
    "model = LanNetBinary(cfg[\"input_dim\"], cfg[\"hidden_dim\"], cfg[\"num_layers\"])\n",
    "sd = load_safetensors(os.path.join(model_dir, \"model.safetensors\"))\n",
    "model.load_state_dict(sd, strict=True)\n",
    "model.eval()\n",
    "\n",
    "# Feature extraction should match preprocessor_config.json\n",
    "def fbanks_from_array(y, sr=pp[\"sampling_rate\"], n_mels=pp[\"n_mels\"], n_fft=pp[\"n_fft\"], hop_length=pp[\"hop_length\"], max_len=pp[\"max_len_frames\"]):\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length, power=2.0)\n",
    "    fbanks = librosa.power_to_db(mel).T\n",
    "    T = fbanks.shape[0]\n",
    "    if T < max_len:\n",
    "        import numpy as np\n",
    "        fbanks = np.pad(fbanks, ((0, max_len - T), (0, 0)), mode=\"constant\")\n",
    "    else:\n",
    "        fbanks = fbanks[:max_len, :]\n",
    "    return torch.tensor(fbanks, dtype=torch.float32).unsqueeze(0)  # (1, T, F)\n",
    "\n",
    "# Example: predict from a waveform array \"y\" at 16kHz\n",
    "# y, _ = librosa.load(\"example.wav\", sr=pp[\"sampling_rate\"])\n",
    "# x = fbanks_from_array(y)\n",
    "# with torch.no_grad():\n",
    "#     logits = model(x)\n",
    "#     pred = int(torch.argmax(logits, dim=1))\n",
    "#     print(lm[str(pred)])\n",
    "\"\"\"\n",
    "with open(os.path.join(MODEL_DIR_EFS, \"README.md\"), \"w\") as f:\n",
    "    f.write(model_readme)\n",
    "\n",
    "\n",
    "# --------- 6) Copy dataset splits & write dataset README ---------\n",
    "shutil.copy2(TRAIN_SRC, os.path.join(DS_DIR_EFS, \"data\", \"train.parquet\"))\n",
    "shutil.copy2(TEST_SRC, os.path.join(DS_DIR_EFS, \"data\", \"test.parquet\"))\n",
    "ds_readme = f\"\"\"# {DS_NAME} dataset\n",
    "\n",
    "Train/test splits for Shanghai vs Not-Shanghai binary classification.\n",
    "Contents\n",
    "data/train.parquet\n",
    "data/test.parquet\n",
    "Each row contains:\n",
    "audio: float array (mono)\n",
    "sampling_rate: 16000\n",
    "dialect/label: label (Shanghai=1, else 0)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(DS_DIR_EFS, \"README.md\"), \"w\") as f:\n",
    "    f.write(ds_readme)\n",
    "    print(\"Export complete:\")\n",
    "    print(\"Model dir:\", MODEL_DIR_EFS)\n",
    "    print(\"Dataset dir:\", DS_DIR_EFS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ee00bc-1103-4f09-b980-9c9ee6f31735",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T02:46:17.509530Z",
     "iopub.status.busy": "2025-09-15T02:46:17.509317Z",
     "iopub.status.idle": "2025-09-15T02:46:18.258797Z",
     "shell.execute_reply": "2025-09-15T02:46:18.258240Z",
     "shell.execute_reply.started": "2025-09-15T02:46:17.509515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911M\t/mnt/custom-file-systems/efs/fs-0a84517bf3cf54d59_fsap-04bd72a3f345b82c0/dialect-modeling/hf\n",
      "9.7M\t/mnt/custom-file-systems/efs/fs-0a84517bf3cf54d59_fsap-04bd72a3f345b82c0/dialect-modeling/hf/models\n",
      "901M\t/mnt/custom-file-systems/efs/fs-0a84517bf3cf54d59_fsap-04bd72a3f345b82c0/dialect-modeling/hf/datasets\n",
      "911M\t/mnt/custom-file-systems/efs/fs-0a84517bf3cf54d59_fsap-04bd72a3f345b82c0/dialect-modeling/hf\n",
      "/mnt/custom-file-systems/efs/fs-0a84517bf3cf54d59_fsap-04bd72a3f345b82c0/dialect-modeling/hf:\n",
      "total 8.0K\n",
      "drwxr-xr-x 3 nobody nogroup 6.0K Sep 15 02:43 datasets\n",
      "drwxr-xr-x 3 nobody nogroup 6.0K Sep 15 02:43 models\n",
      "\n",
      "/mnt/custom-file-systems/efs/fs-0a84517bf3cf54d59_fsap-04bd72a3f345b82c0/dialect-modeling/hf/datasets:\n",
      "total 4.0K\n",
      "drwxr-xr-x 3 nobody nogroup 6.0K Sep 15 02:43 shanghai-binary\n",
      "\n",
      "/mnt/custom-file-systems/efs/fs-0a84517bf3cf54d59_fsap-04bd72a3f345b82c0/dialect-modeling/hf/datasets/shanghai-binary:\n",
      "total 8.0K\n",
      "-rw-r--r-- 1 nobody nogroup  252 Sep 15 02:43 README.md\n",
      "drwxr-xr-x 2 nobody nogroup 6.0K Sep 15 02:43 data\n",
      "\n",
      "/mnt/custom-file-systems/efs/fs-0a84517bf3cf54d59_fsap-04bd72a3f345b82c0/dialect-modeling/hf/datasets/shanghai-binary/data:\n",
      "total 901M\n",
      "-rw-r--r-- 1 nobody nogroup 9.6M Sep 14 18:05 test.parquet\n",
      "-rw-r--r-- 1 nobody nogroup 892M Sep 14 18:05 train.parquet\n",
      "\n",
      "/mnt/custom-file-systems/efs/fs-0a84517bf3cf54d59_fsap-04bd72a3f345b82c0/dialect-modeling/hf/models:\n",
      "total 4.0K\n",
      "drwxr-xr-x 2 nobody nogroup 6.0K Sep 15 02:43 shanghai-binary\n",
      "\n",
      "/mnt/custom-file-systems/efs/fs-0a84517bf3cf54d59_fsap-04bd72a3f345b82c0/dialect-modeling/hf/models/shanghai-binary:\n",
      "total 9.7M\n",
      "-rw-r--r-- 1 nobody nogroup 2.4K Sep 15 02:43 README.md\n",
      "-rw-r--r-- 1 nobody nogroup  227 Sep 15 02:43 config.json\n",
      "-rw-r--r-- 1 nobody nogroup   44 Sep 15 02:43 label_mapping.json\n",
      "-rw-r--r-- 1 nobody nogroup 9.7M Sep 15 02:43 model.safetensors\n",
      "-rw-r--r-- 1 nobody nogroup  167 Sep 15 02:43 preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "!du -sh /mnt/custom-file-systems/efs/fs-0a84517bf3cf54d59_fsap-04bd72a3f345b82c0/dialect-modeling/hf\n",
    "!du -h --max-depth=1 /mnt/custom-file-systems/efs/fs-0a84517bf3cf54d59_fsap-04bd72a3f345b82c0/dialect-modeling/hf\n",
    "!ls -lhR /mnt/custom-file-systems/efs/fs-0a84517bf3cf54d59_fsap-04bd72a3f345b82c0/dialect-modeling/hf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e2ae3b-5672-4f4d-a886-84ff9caa97c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
