{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3064000",
   "metadata": {},
   "source": [
    "## add generated transcription (mandarin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1794f35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ASR Unit Test: 100%|██████████| 10/10 [00:07<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shanghai WER: 1.000\n",
      "Shanghai CER: 0.894\n",
      "No Mandarin samples\n",
      "No Mandarin samples\n",
      "Label: 0, Gender: male\n",
      "Reference: 北京爱数智慧语音采集\n",
      "Prediction: 不近也书资会许域切间\n",
      "----------------------------------------\n",
      "Label: 0, Gender: female\n",
      "Reference: 北京爱数智慧语音采集\n",
      "Prediction: 破听UNK手智慧理且救\n",
      "----------------------------------------\n",
      "Label: 0, Gender: male\n",
      "Reference: 阿拉两个拧来聊聊金融方面呃\n",
      "Prediction: 安了脸岸历列有着金用方闭的\n",
      "----------------------------------------\n",
      "Label: 0, Gender: male\n",
      "Reference: 金融方面嘛\n",
      "Prediction: 真入发给外来\n",
      "----------------------------------------\n",
      "Label: 0, Gender: male\n",
      "Reference: 搿呃阿姨喃应该讲侬已经交关年数辣辣了解了\n",
      "Prediction: 和爱意的应给段奴京这被理素了了表加来\n",
      "----------------------------------------\n",
      "Label: 0, Gender: male\n",
      "Reference: 葛末吾辣辣金融方面已经有的三四年了\n",
      "Prediction: 跟么无来的近方比经的谢死年了\n",
      "----------------------------------------\n",
      "Label: 0, Gender: male\n",
      "Reference: 最少辰光阿拉是做撒呃喃有钞票就是到银行里保本保息\n",
      "Prediction: 据说有猛光阿来主杀呢有扯破了子斗你那里薄泵抱士\n",
      "----------------------------------------\n",
      "Label: 0, Gender: female\n",
      "Reference: 吾已经做了已经到八七年了\n",
      "Prediction: 我经足累经的霸劝一来\n",
      "----------------------------------------\n",
      "Label: 0, Gender: female\n",
      "Reference: 八七年呃当时辰光辣里哴相做理财呃\n",
      "Prediction: 把且你的当子管了李案战出理耶\n",
      "----------------------------------------\n",
      "Label: 0, Gender: female\n",
      "Reference: 是两级风险三级风险四级风险吾侪做呃信托咯撒侪做呃\n",
      "Prediction: 两这不系非界控析私界不雄也组了心托老少也除\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from jiwer import wer, cer\n",
    "\n",
    "MODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\"\n",
    "CHARS_TO_IGNORE = [\",\", \"?\", \"¿\", \".\", \"!\", \"¡\", \";\", \"；\", \":\", '\"\"', \"%\", '\"', \"�\", \"ʿ\", \"·\", \"჻\", \"~\", \"՞\",\n",
    "                  \"؟\", \"،\", \"।\", \"॥\", \"«\", \"»\", \"„\", \"“\", \"”\", \"「\", \"」\", \"‘\", \"’\", \"《\", \"》\", \"(\", \")\", \"[\", \"]\",\n",
    "                  \"{\", \"}\", \"=\", \"`\", \"_\", \"+\", \"<\", \">\", \"…\", \"–\", \"°\", \"´\", \"ʾ\", \"‹\", \"›\", \"©\", \"®\", \"—\", \"→\", \"。\",\n",
    "                  \"、\", \"﹂\", \"﹁\", \"‧\", \"～\", \"﹏\", \"，\", \"｛\", \"｝\", \"（\", \"）\", \"［\", \"］\", \"【\", \"】\", \"‥\", \"〽\",\n",
    "                  \"『\", \"』\", \"〝\", \"〟\", \"⟨\", \"⟩\", \"〜\", \"：\", \"！\", \"？\", \"♪\", \"；\", \"/\", \"\\\\\", \"º\", \"−\", \"^\", \"'\", \"ʻ\", \"ˆ\"]\n",
    "chars_to_ignore_regex = f\"[{re.escape(''.join(CHARS_TO_IGNORE))}]\"\n",
    "\n",
    "# Load ASR model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "asr_model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n",
    "asr_model.eval()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove special characters and convert to uppercase for fair comparison\n",
    "    return re.sub(chars_to_ignore_regex, \"\", text).upper()\n",
    "\n",
    "def transcribe(audio_array, sampling_rate):\n",
    "    # Resample to 16kHz if needed\n",
    "    if sampling_rate != 16000:\n",
    "        audio_array = librosa.resample(audio_array, orig_sr=sampling_rate, target_sr=16000)\n",
    "        sampling_rate = 16000\n",
    "    # Normalize audio\n",
    "    if audio_array.dtype != 'float32':\n",
    "        audio_array = audio_array.astype('float32')\n",
    "    # Prepare input\n",
    "    input_values = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\").input_values\n",
    "    with torch.no_grad():\n",
    "        logits = asr_model(input_values).logits\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(pred_ids)[0]\n",
    "    return transcription.strip()\n",
    "\n",
    "# Build new dataset with reference, prediction, label, and gender\n",
    "asr_results = []\n",
    "for audio, sampling_rate, label, text, gender in tqdm(combined_data_with_gender[:10], desc=\"ASR Unit Test\"):  # Use [:10] for unit test, remove for full dataset\n",
    "    pred_text = transcribe(audio, sampling_rate)\n",
    "    asr_results.append({\n",
    "        \"reference\": clean_text(text),\n",
    "        \"prediction\": clean_text(pred_text),\n",
    "        \"label\": label,\n",
    "        \"gender\": gender\n",
    "    })\n",
    "\n",
    "# Separate Shanghai and Mandarin results\n",
    "shanghai_refs = [row[\"reference\"] for row in asr_results if row[\"label\"] == 0]\n",
    "shanghai_preds = [row[\"prediction\"] for row in asr_results if row[\"label\"] == 0]\n",
    "mandarin_refs = [row[\"reference\"] for row in asr_results if row[\"label\"] == 1]\n",
    "mandarin_preds = [row[\"prediction\"] for row in asr_results if row[\"label\"] == 1]\n",
    "\n",
    "# Compute metrics\n",
    "shanghai_wer = wer(shanghai_refs, shanghai_preds) if shanghai_refs else None\n",
    "shanghai_cer = cer(shanghai_refs, shanghai_preds) if shanghai_refs else None\n",
    "mandarin_wer = wer(mandarin_refs, mandarin_preds) if mandarin_refs else None\n",
    "mandarin_cer = cer(mandarin_refs, mandarin_preds) if mandarin_refs else None\n",
    "\n",
    "print(f\"Shanghai WER: {shanghai_wer:.3f}\" if shanghai_wer is not None else \"No Shanghai samples\")\n",
    "print(f\"Shanghai CER: {shanghai_cer:.3f}\" if shanghai_cer is not None else \"No Shanghai samples\")\n",
    "print(f\"Mandarin WER: {mandarin_wer:.3f}\" if mandarin_wer is not None else \"No Mandarin samples\")\n",
    "print(f\"Mandarin CER: {mandarin_cer:.3f}\" if mandarin_cer is not None else \"No Mandarin samples\")\n",
    "\n",
    "# Optional: inspect results\n",
    "for row in asr_results:\n",
    "    print(f\"Label: {row['label']}, Gender: {row['gender']}\")\n",
    "    print(f\"Reference: {row['reference']}\")\n",
    "    print(f\"Prediction: {row['prediction']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff64d11",
   "metadata": {},
   "source": [
    "## Run pre-trained classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "018100e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Combined Dataset: 100%|██████████| 6792/6792 [00:35<00:00, 190.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to extract features\n",
    "def extract_features(audio, sr, n_mfcc=40):\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "    return np.transpose(mfccs)  # Transpose to match (time, features) format\n",
    "\n",
    "# Preprocess the combined dataset with tqdm\n",
    "processed_data = []\n",
    "for audio, sr, label in tqdm(combined_data, desc=\"Processing Combined Dataset\"):\n",
    "    features = extract_features(audio, sr)\n",
    "    processed_data.append((features, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2c50a251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "# Check the labels in processed_data\n",
    "labels = [label for _, label in processed_data]\n",
    "print(set(labels))  # Should print {0, 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "device = torch.device(\"cpu\")\n",
    "#force CPU usage; no GPU available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c68b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load('model9.model', map_location=device)\n",
    "#download from https://github.com/Colt1990/chinese-dialect-recognition/blob/master/model9.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4ed4474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['layer1.GRU.weight_ih_l0', 'layer1.GRU.weight_hh_l0', 'layer1.GRU.bias_ih_l0', 'layer1.GRU.bias_hh_l0', 'layer1.GRU.weight_ih_l1', 'layer1.GRU.weight_hh_l1', 'layer1.GRU.bias_ih_l1', 'layer1.GRU.bias_hh_l1', 'layer2.linear.weight', 'layer2.linear.bias', 'layer3.linear.weight', 'layer3.linear.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9f2054ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Prepare tensors\n",
    "features = [torch.tensor(f, dtype=torch.float32) for f, _ in processed_data]\n",
    "labels = [torch.tensor(l, dtype=torch.long) for _, l in processed_data]\n",
    "\n",
    "# Pad sequences to the same length\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "padded_features = pad_sequence(features, batch_first=True)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataset = TensorDataset(padded_features, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "426ef151",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LanNet Model: \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class LanNet(nn.Module):\n",
    "    def __init__(self, input_dim=40, hidden_dim=512, bn_dim=192, output_dim=2):\n",
    "        super(LanNet, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bn_dim = bn_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.layer1 = nn.Sequential()\n",
    "        self.layer1.add_module('GRU', nn.GRU(self.input_dim, self.hidden_dim, num_layers=2, batch_first=True, bidirectional=False))\n",
    "\t\n",
    "\t\n",
    "        self.layer2 = nn.Sequential()\n",
    "        self.layer2.add_module('linear', nn.Linear(self.hidden_dim, self.bn_dim))\n",
    "\n",
    "        self.layer3 = nn.Sequential()\n",
    "        self.layer3.add_module('linear', nn.Linear(self.bn_dim,self.output_dim))\n",
    "\n",
    "    # def forward(self, src, mask, target):\n",
    "    #     batch_size, fea_frames, fea_dim = src.size()\n",
    "\n",
    "    #     out_hidden, hidd = self.layer1(src)\n",
    "    #     out_hidden = out_hidden.contiguous().view(-1, out_hidden.size(-1))   \n",
    "    #     out_bn = self.layer2(out_hidden)\n",
    "    #     out_target = self.layer3(out_bn)\n",
    "\n",
    "\n",
    "    #     out_target = out_target.contiguous().view(batch_size, fea_frames, -1)\n",
    "    #     mask = mask.contiguous().view(batch_size, fea_frames, 1).expand(batch_size, fea_frames, out_target.size(2))\n",
    "    #     out_target_mask = out_target * mask\n",
    "    #     out_target_mask = out_target_mask.sum(dim=1)/mask.sum(dim=1)\n",
    "    #     predict_target = F.softmax(out_target_mask, dim=1)\n",
    "\n",
    "    #     # Reshape target to match the dimensions of predict_target\n",
    "    #     target = target.view(-1, 1)  # Reshape to [batch_size, 1]\n",
    "\t\n",
    "\n",
    "    #     # 计算loss\n",
    "    #     tar_select_new = torch.gather(predict_target, 1, target)\n",
    "    #     ce_loss = -torch.log(tar_select_new) \n",
    "    #     ce_loss = ce_loss.sum() / batch_size\n",
    "\n",
    "    #     # 计算acc\n",
    "    #     _, predict = predict_target.max(dim=1)\n",
    "    #     predict = predict.contiguous().view(-1,1)\n",
    "    #     correct = predict.eq(target).float()       \n",
    "    #     num_samples = predict.size(0)\n",
    "    #     sum_acc = correct.sum().item()\n",
    "    #     acc = sum_acc/num_samples\n",
    "\n",
    "    #     return acc, ce_loss\n",
    "    \n",
    "    def forward(self, src, mask, target=None):\n",
    "        batch_size, fea_frames, fea_dim = src.size()\n",
    "\n",
    "        out_hidden, hidd = self.layer1(src)\n",
    "        out_hidden = out_hidden.contiguous().view(-1, out_hidden.size(-1))   \n",
    "        out_bn = self.layer2(out_hidden)\n",
    "        out_target = self.layer3(out_bn)\n",
    "\n",
    "        out_target = out_target.contiguous().view(batch_size, fea_frames, -1)\n",
    "        mask = mask.contiguous().view(batch_size, fea_frames, 1).expand(batch_size, fea_frames, out_target.size(2))\n",
    "        out_target_mask = out_target * mask\n",
    "        out_target_mask = out_target_mask.sum(dim=1) / mask.sum(dim=1)\n",
    "        predict_target = F.softmax(out_target_mask, dim=1)\n",
    "\n",
    "        if target is None:\n",
    "            # During evaluation, return only the predicted probabilities\n",
    "            return predict_target\n",
    "\n",
    "        # Reshape target to match the dimensions of predict_target\n",
    "        target = target.view(-1, 1)  # Reshape to [batch_size, 1]\n",
    "\n",
    "        # Compute loss\n",
    "        tar_select_new = torch.gather(predict_target, 1, target)\n",
    "        ce_loss = -torch.log(tar_select_new) \n",
    "        ce_loss = ce_loss.sum() / batch_size\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predict = predict_target.max(dim=1)\n",
    "        predict = predict.contiguous().view(-1, 1)\n",
    "        correct = predict.eq(target).float()\n",
    "        num_samples = predict.size(0)\n",
    "        sum_acc = correct.sum().item()\n",
    "        acc = sum_acc / num_samples\n",
    "\n",
    "        return acc, ce_loss\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "77ed1aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanNet(\n",
       "  (layer1): Sequential(\n",
       "    (GRU): GRU(40, 512, num_layers=2, batch_first=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (linear): Linear(in_features=512, out_features=192, bias=True)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (linear): Linear(in_features=192, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the LanNet model\n",
    "model = LanNet(input_dim=40, hidden_dim=512, bn_dim=192, output_dim=2)  # 2 classes: Shanghai and Mandarin\n",
    "state_dict = torch.load('model9.model', map_location='cpu')\n",
    "\n",
    "# Remove the weights for the final layer (layer3.linear) from the state_dict since we have only 2 classes\n",
    "state_dict.pop('layer3.linear.weight')\n",
    "state_dict.pop('layer3.linear.bias')\n",
    "\n",
    "model.load_state_dict(state_dict, strict=False)  # Use strict=False to ignore missing keys\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e9b9cd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Batches: 100%|██████████| 425/425 [01:18<00:00,  5.44it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform classification with tqdm\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_labels in tqdm(dataloader, desc=\"Classifying Batches\"):\n",
    "        mask = torch.ones(batch_features.size(0), batch_features.size(1), dtype=torch.float32)  # Create masks\n",
    "        predictions, _ = model(batch_features, mask, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fd6d1002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5412288a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Accuracy: 100%|██████████| 425/425 [01:15<00:00,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# # Initialize variables to track accuracy\n",
    "# correct_predictions = 0\n",
    "# total_samples = 0\n",
    "\n",
    "# # Perform classification and calculate accuracy\n",
    "# with torch.no_grad():\n",
    "#     for batch_features, batch_labels in tqdm(dataloader, desc=\"Evaluating Accuracy\"):\n",
    "#         # Create masks (all ones, assuming no padding)\n",
    "#         mask = torch.ones(batch_features.size(0), batch_features.size(1), dtype=torch.float32)\n",
    "\n",
    "#         # Get predictions from the model\n",
    "#         predictions, _ = model(batch_features, mask, batch_labels)\n",
    "\n",
    "#         # Get the predicted classes\n",
    "#         predicted_classes = torch.argmax(predictions, dim=1)  # Shape: [batch_size]\n",
    "\n",
    "#         # Compare with ground truth labels\n",
    "#         correct_predictions += (predicted_classes == batch_labels).sum().item()\n",
    "#         total_samples += batch_labels.size(0)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = correct_predictions / total_samples\n",
    "# print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize variables to track accuracy\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Perform classification and calculate accuracy\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_labels in tqdm(dataloader, desc=\"Evaluating Accuracy\"):\n",
    "        # Ensure batch_labels has the correct shape and type\n",
    "        batch_labels = batch_labels.view(-1)  # Flatten to [batch_size]\n",
    "        batch_labels = batch_labels.long()  # Ensure type is torch.long\n",
    "\n",
    "        # Create masks (all ones, assuming no padding)\n",
    "        mask = torch.ones(batch_features.size(0), batch_features.size(1), dtype=torch.float32)\n",
    "\n",
    "        # Get predictions from the model\n",
    "        predictions = model(batch_features, mask)  # Only predicted probabilities are returned\n",
    "\n",
    "        # Get the predicted classes\n",
    "        predicted_classes = torch.argmax(predictions, dim=1)  # Shape: [batch_size]\n",
    "\n",
    "        # Compare with ground truth labels\n",
    "        correct_predictions += (predicted_classes == batch_labels).sum().item()\n",
    "        total_samples += batch_labels.size(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d14fa8",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "\n",
    "Note that the dialect data sizes are similar. Thus, our accuracy is no better than a coin flip. Unfortunate. \n",
    "\n",
    "1. Need to sort out features and break out gender, length of audio, age to see if impacts predictive accuracy/classification\n",
    "2. Fine-tune and adjust parameters to find improvements\n",
    "3. Implement denoising model on all to see if improves \n",
    "4. Add additional classes (dialects) to see if classification improves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd6466b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test fine-tuning\n",
    "\n",
    "# # Fine-tune the model\n",
    "# model.train()\n",
    "# for epoch in range(10):  # Number of epochs\n",
    "#     for batch_features, batch_labels in dataloader:\n",
    "#         # Ensure batch_labels has the correct shape and type\n",
    "#         batch_labels = batch_labels.view(-1, 1)  # Reshape to [batch_size, 1]\n",
    "#         batch_labels = batch_labels.long()  # Ensure type is torch.long\n",
    "\n",
    "#         # Create a mask (all ones, as no padding is applied here)\n",
    "#         mask = torch.ones(batch_features.size(0), batch_features.size(1), dtype=torch.float32)\n",
    "\n",
    "#         # Forward pass\n",
    "#         _, loss = model(batch_features, mask, batch_labels)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af05ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63a13e71",
   "metadata": {},
   "source": [
    "## Analyze linguistic components "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c3890",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
