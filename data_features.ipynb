{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69302d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import tempfile\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "from transformers import BertTokenizer, ErnieModel\n",
    "from tqdm import tqdm\n",
    "from pyannote.audio import Model, Inference\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1c3f70",
   "metadata": {},
   "source": [
    "# Add Features \n",
    "\n",
    "generates the following (if doesn't already exist): gender, snr (signal-to-noise ratio), tokens, sentiment/emotion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab3cab",
   "metadata": {},
   "source": [
    "### Add gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68b093c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model and processor\n",
    "model_name = \"prithivMLmods/Common-Voice-Gender-Detection\"\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Label mapping\n",
    "id2label = {\n",
    "    0: \"female\",\n",
    "    1: \"male\"\n",
    "}\n",
    "\n",
    "def predict_gender(audio_array, sampling_rate):\n",
    "    # all audio arrays should be 16kHz, so we don't need to resample\n",
    "    # Prepare input\n",
    "    inputs = processor(audio_array, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        pred_id = logits.argmax(dim=-1).item()\n",
    "    return id2label[pred_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc549712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gender Detection: 100%|██████████| 6200/6200 [13:45<00:00,  7.51it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: shanghai, Text: 北京爱数智慧语音采集, Length: 3.98s, Gender: male\n",
      "Label: shanghai, Text: 北京爱数智慧语音采集, Length: 2.69s, Gender: female\n",
      "Label: shanghai, Text: 阿拉两个拧来聊聊金融方面呃, Length: 2.66s, Gender: male\n",
      "Label: shanghai, Text: 金融方面嘛, Length: 1.32s, Gender: male\n",
      "Label: shanghai, Text: 搿呃，阿姨喃，应该讲，侬已经交关年数辣辣了解了, Length: 5.37s, Gender: male\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply gender prediction only to rows where gender is None (i.e., not 'shanghai')\n",
    "\n",
    "combined_data_with_gender = []\n",
    "for audio, sampling_rate, label, text, audio_length, gender in tqdm(loaded_data, desc=\"Gender Detection\"):\n",
    "    # gender is already present for 'shanghai'), keep it\n",
    "    if gender is not None:\n",
    "        combined_data_with_gender.append((audio, sampling_rate, label, text, audio_length, gender))\n",
    "    else:\n",
    "        # Predict gender for other dialects\n",
    "        predicted_gender = predict_gender(audio, sampling_rate)\n",
    "        combined_data_with_gender.append((audio, sampling_rate, label, text, audio_length, predicted_gender))\n",
    "\n",
    "# Now each tuple is (audio, sampling_rate, label, text, audio_length, gender)\n",
    "# print the first 5 with gender\n",
    "for row in combined_data_with_gender[:5]:\n",
    "    print(f\"Label: {row[2]}, Text: {row[3][:30]}, Length: {row[4]:.2f}s, Gender: {row[5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8b4e61",
   "metadata": {},
   "source": [
    "### Add tokens from transcription (sichuan does not have this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d041c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ErnieModel were not initialized from the model checkpoint at nghuyong/ernie-3.0-nano-zh and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"nghuyong/ernie-3.0-nano-zh\")\n",
    "model = ErnieModel.from_pretrained(\"nghuyong/ernie-3.0-nano-zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2add8c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data_with_tokens = []\n",
    "for audio, sampling_rate, label, text, audio_length, gender in combined_data_with_gender: \n",
    "    token = tokenizer(text, padding='max_length', truncation=True, max_length = 128, return_tensors=\"pt\")\n",
    "    combined_data_with_tokens.append((audio, sampling_rate, label, text, audio_length, gender, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3817ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'北 京 爱 数 智 慧 语 音 采 集'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test = combined_data_with_tokens[:1][0][-1]  # Show the token for the first entry\n",
    "# tokenizer.decode(test.input_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6101f8a0",
   "metadata": {},
   "source": [
    "## Add sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36033b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9551, 0.0449]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#load and test \n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment')\n",
    "model=BertForSequenceClassification.from_pretrained('IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment')\n",
    "\n",
    "text='今天心情不好'\n",
    "\n",
    "output=model(torch.tensor([tokenizer.encode(text)]))\n",
    "print(torch.nn.functional.softmax(output.logits,dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59c2f0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis: 100%|██████████| 6200/6200 [05:42<00:00, 18.09it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128, padding=\"max_length\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        sentiment_id = torch.argmax(probs, dim=-1).item()\n",
    "    sentiment_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "    return sentiment_map.get(sentiment_id, \"unknown\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "combined_data_with_sentiment = []\n",
    "for audio, sampling_rate, label, text, audio_length, gender, token in tqdm(combined_data_with_tokens, desc=\"Sentiment Analysis\"):\n",
    "    sentiment = get_sentiment(text)\n",
    "    combined_data_with_sentiment.append((audio, sampling_rate, label, text, audio_length, gender, token, sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74586d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 北京爱数智慧语音采集, Audio Length: 3.98, Gender: male, Sentiment: neutral\n",
      "Text: 北京爱数智慧语音采集, Audio Length: 2.69, Gender: female, Sentiment: neutral\n",
      "Text: 阿拉两个拧来聊聊金融方面呃, Audio Length: 2.66, Gender: male, Sentiment: neutral\n"
     ]
    }
   ],
   "source": [
    "# # Example: print first 3 rows with sentiment\n",
    "# for row in combined_data_with_sentiment[:3]:\n",
    "#     print(f\"Text: {row[3][:30]}, Audio Length: {row[4]}, Gender: {row[5]}, Sentiment: {row[7]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "768812a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment distribution:\n",
      "neutral: 2936\n",
      "negative: 3264\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Extract sentiment labels from combined_data_with_sentiment\n",
    "sentiments = [row[7] for row in combined_data_with_sentiment]\n",
    "sentiment_counts = Counter(sentiments)\n",
    "\n",
    "print(\"Sentiment distribution:\")\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    print(f\"{sentiment}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b6804",
   "metadata": {},
   "source": [
    "#### Alternative sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from funasr import AutoModel\n",
    "\n",
    "# # Load the emotion2vec_plus_base model\n",
    "# model = AutoModel(model=\"iic/emotion2vec_plus_base\")\n",
    "\n",
    "# # Run inference on a sample of 10 audio samples in combined_data_with_tokens\n",
    "# results = []\n",
    "# sample_size = 10\n",
    "# for i, (audio, sampling_rate, label, text, audio_length, gender, token) in enumerate(combined_data_with_tokens[:sample_size]):\n",
    "#     import soundfile as sf\n",
    "#     import tempfile\n",
    "#     with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_wav:\n",
    "#         sf.write(tmp_wav.name, audio, sampling_rate)\n",
    "#         wav_file = tmp_wav.name\n",
    "#         # Run inference\n",
    "#         res = model.generate(wav_file, output_dir=None, granularity=\"utterance\", extract_embedding=False)\n",
    "#         results.append((label, text, res))\n",
    "#     import os\n",
    "#     os.remove(wav_file)\n",
    "\n",
    "# # Print the results\n",
    "# for r in results:\n",
    "#     label, text, res = r\n",
    "#     print(f\"Label: {label}, Text: {text[:30]}, Emotion: {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10072d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: shanghai, Text: 北京爱数智慧语音采集, Top Emotion: 中立/neutral, Score: 1.0000\n",
      "Label: shanghai, Text: 北京爱数智慧语音采集, Top Emotion: 中立/neutral, Score: 1.0000\n",
      "Label: shanghai, Text: 阿拉两个拧来聊聊金融方面呃, Top Emotion: 中立/neutral, Score: 0.6006\n",
      "Label: shanghai, Text: 金融方面嘛, Top Emotion: 中立/neutral, Score: 0.5899\n",
      "Label: shanghai, Text: 搿呃，阿姨喃，应该讲，侬已经交关年数辣辣了解了, Top Emotion: 中立/neutral, Score: 1.0000\n",
      "Label: shanghai, Text: 葛末，吾辣辣金融方面已经有的三四年了, Top Emotion: 中立/neutral, Score: 1.0000\n",
      "Label: shanghai, Text: 最少辰光阿拉是做撒呃喃，有钞票就是到银行里保本保息, Top Emotion: 中立/neutral, Score: 1.0000\n",
      "Label: shanghai, Text: 吾已经做了已经到八七年了, Top Emotion: 中立/neutral, Score: 1.0000\n",
      "Label: shanghai, Text: 八七年呃，当时辰光辣里哴相做理财呃, Top Emotion: 中立/neutral, Score: 0.9999\n",
      "Label: shanghai, Text: 是两级风险，三级风险，四级风险吾侪做呃，信托咯撒侪做呃, Top Emotion: 中立/neutral, Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# # Print just the highest emotion label and score for each result in results\n",
    "# for entry in results:\n",
    "#     label, text, emotions = entry\n",
    "#     if isinstance(emotions, list) and len(emotions) > 0 and \"labels\" in emotions[0] and \"scores\" in emotions[0]:\n",
    "#         labels = emotions[0][\"labels\"]\n",
    "#         scores = emotions[0][\"scores\"]\n",
    "#         max_idx = scores.index(max(scores))\n",
    "#         top_emotion = labels[max_idx]\n",
    "#         top_score = scores[max_idx]\n",
    "#         print(f\"Label: {label}, Text: {text[:30]}, Top Emotion: {top_emotion}, Score: {top_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e489dfe",
   "metadata": {},
   "source": [
    "## Add SNR (signal to noise ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a570c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. visit hf.co/pyannote/brouhaha and accept user conditions\n",
    "# 2. visit hf.co/settings/tokens to create an access token\n",
    "# 3. instantiate pretrained model\n",
    "\n",
    "from pyannote.audio import Model, Inference\n",
    "model = Model.from_pretrained(\"pyannote/brouhaha\", \n",
    "                              use_auth_token=\"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\")\n",
    "\n",
    "# Create inference object\n",
    "inference = Inference(model)\n",
    "\n",
    "def extract_snr_from_audio(audio_array, sampling_rate):\n",
    "    \"\"\"\n",
    "    Extract SNR from audio array using pyannote.audio brouhaha model\n",
    "    \n",
    "    Args:\n",
    "        audio_array: numpy array of audio data\n",
    "        sampling_rate: sampling rate of the audio\n",
    "    \n",
    "    Returns:\n",
    "        float: average SNR value\n",
    "    \"\"\"\n",
    "    # Create a temporary WAV file\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_wav:\n",
    "        # Write audio array to temporary file\n",
    "        sf.write(tmp_wav.name, audio_array, sampling_rate)\n",
    "        \n",
    "        try:\n",
    "            # Apply model inference\n",
    "            output = inference(tmp_wav.name)\n",
    "            \n",
    "            # Extract SNR values\n",
    "            snr_values = []\n",
    "            for frame, (vad, snr, c50) in output:\n",
    "                snr_values.append(snr)\n",
    "            \n",
    "            # Calculate average SNR (you can modify this to use median, max, etc.)\n",
    "            avg_snr = np.mean(snr_values) if snr_values else 0.0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing audio: {e}\")\n",
    "            avg_snr = 0.0\n",
    "        \n",
    "        finally:\n",
    "            # Clean up temporary file\n",
    "            os.unlink(tmp_wav.name)\n",
    "    \n",
    "    return avg_snr\n",
    "\n",
    "\n",
    "def add_snr_to_dataset(combined_data_with_sentiment):\n",
    "    \"\"\"\n",
    "    Add SNR feature to existing dataset\n",
    "    \n",
    "    Args:\n",
    "        combined_data_with_sentiment: your existing dataset with audio arrays\n",
    "    \n",
    "    Returns:\n",
    "        list: final_data with SNR added\n",
    "    \"\"\"\n",
    "    final_data = []\n",
    "    \n",
    "    print(f\"Processing {len(combined_data_with_sentiment)} audio samples...\")\n",
    "    \n",
    "    for i, row in enumerate(combined_data_with_sentiment):\n",
    "        # Extract existing data\n",
    "        audio, sampling_rate, label, text, audio_length, gender, token, sentiment = row\n",
    "        \n",
    "        # Extract SNR from the audio array\n",
    "        snr = extract_snr_from_audio(audio, sampling_rate)\n",
    "        \n",
    "        # Add SNR to the row\n",
    "        updated_row = (audio, sampling_rate, label, text, audio_length, gender, token, sentiment, snr)\n",
    "        final_data.append(updated_row)\n",
    "        \n",
    "        # Print progress for first few items and every 100th item\n",
    "        if i < 3 or i % 100 == 0:\n",
    "            print(f\"Processed item {i}: SNR = {snr:.2f} dB\")\n",
    "    \n",
    "    print(f\"Completed! Added SNR to {len(final_data)} samples.\")\n",
    "    return final_data\n",
    "\n",
    "# Test with just one instance first\n",
    "print(\"=== Testing with one instance ===\")\n",
    "test_row = combined_data_with_sentiment[0]\n",
    "audio, sampling_rate, label, text, audio_length, gender, token, sentiment = test_row\n",
    "\n",
    "print(f\"Original row length: {len(test_row)}\")\n",
    "print(f\"Audio data type: {type(audio)}\")\n",
    "print(f\"Audio data shape: {audio.shape if hasattr(audio, 'shape') else 'No shape attribute'}\")\n",
    "print(f\"Sampling rate: {sampling_rate}\")\n",
    "\n",
    "# Extract SNR for the test instance\n",
    "snr = extract_snr_from_audio(audio, sampling_rate)\n",
    "print(f\"Extracted SNR: {snr:.2f} dB\")\n",
    "\n",
    "# Create updated row\n",
    "updated_test_row = (audio, sampling_rate, label, text, audio_length, gender, token, sentiment, snr)\n",
    "print(f\"Updated row length: {len(updated_test_row)}\")\n",
    "\n",
    "# If the test works, process the entire dataset\n",
    "print(\"\\n=== Processing entire dataset ===\")\n",
    "final_data = add_snr_to_dataset(combined_data_with_sentiment)\n",
    "\n",
    "# Update your columns list\n",
    "updated_columns = [\"audio\", \"sampling_rate\", \"label\", \"text\", \"audio_length\", \"gender\", \"token\", \"sentiment\", \"snr\"]\n",
    "print(f\"\\nUpdated columns: {updated_columns}\")\n",
    "\n",
    "# Verify the final dataset\n",
    "print(f\"\\nFinal dataset size: {len(final_data)}\")\n",
    "print(f\"First row structure: {len(final_data[0])} elements\")\n",
    "print(f\"First row SNR: {final_data[0][8]:.2f} dB\")  # SNR should be at index 8\n",
    "\n",
    "# Optional: Convert to DataFrame if needed\n",
    "import pandas as pd\n",
    "df_final = pd.DataFrame(final_data, columns=updated_columns)\n",
    "print(f\"\\nDataFrame shape: {df_final.shape}\")\n",
    "print(f\"SNR column statistics:\")\n",
    "print(df_final['snr'].describe())\n",
    "\n",
    "# # Example of how to add SNR to your existing data structure\n",
    "# def add_snr_to_dataset(combined_data_with_sentiment, dialect_corpus):\n",
    "#     \"\"\"\n",
    "#     Add SNR feature to existing dataset\n",
    "    \n",
    "#     Args:\n",
    "#         combined_data_with_sentiment: your existing dataset\n",
    "#         dialect_corpus: the original dataset with audio data\n",
    "    \n",
    "#     Returns:\n",
    "#         list: updated dataset with SNR added\n",
    "#     \"\"\"\n",
    "#     updated_data = []\n",
    "    \n",
    "#     for i, row in enumerate(combined_data_with_sentiment):\n",
    "#         # Extract existing data\n",
    "#         audio, sampling_rate, label, text, audio_length, gender, token, sentiment = row\n",
    "        \n",
    "#         # Get the corresponding audio data from the original dataset\n",
    "#         # You'll need to map this correctly based on your data structure\n",
    "#         original_audio_data = dialect_corpus['train'][i]['audio']['array']\n",
    "#         original_sampling_rate = dialect_corpus['train'][i]['audio']['sampling_rate']\n",
    "        \n",
    "#         # Extract SNR\n",
    "#         snr = extract_snr_from_audio(original_audio_data, original_sampling_rate)\n",
    "        \n",
    "#         # Add SNR to the row\n",
    "#         updated_row = (audio, sampling_rate, label, text, audio_length, gender, token, sentiment, snr)\n",
    "#         updated_data.append(updated_row)\n",
    "        \n",
    "#         # Print progress for first few items\n",
    "#         if i < 3:\n",
    "#             print(f\"Processed item {i}: SNR = {snr:.2f} dB\")\n",
    "    \n",
    "#     return updated_data\n",
    "\n",
    "# # Test with just one instance first\n",
    "# print(\"\\n=== Testing with one instance ===\")\n",
    "# test_row = combined_data_with_sentiment[0]  # Assuming this exists\n",
    "# audio, sampling_rate, label, text, audio_length, gender, token, sentiment = test_row\n",
    "\n",
    "# # Get corresponding audio data (you'll need to adjust this mapping)\n",
    "# original_audio_data = dialect_corpus['train'][0]['audio']['array']\n",
    "# original_sampling_rate = dialect_corpus['train'][0]['audio']['sampling_rate']\n",
    "\n",
    "# # Extract SNR\n",
    "# snr = extract_snr_from_audio(original_audio_data, original_sampling_rate)\n",
    "\n",
    "# # Create updated row\n",
    "# updated_test_row = (audio, sampling_rate, label, text, audio_length, gender, token, sentiment, snr)\n",
    "# print(f\"Original row length: {len(test_row)}\")\n",
    "# print(f\"Updated row length: {len(updated_test_row)}\")\n",
    "# print(f\"Added SNR: {snr:.2f} dB\")\n",
    "\n",
    "# # Update your columns list\n",
    "# updated_columns = [\"audio\", \"sampling_rate\", \"label\", \"text\", \"audio_length\", \"gender\", \"token\", \"sentiment\", \"snr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7475f6ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4170bcae",
   "metadata": {},
   "source": [
    "# Rebalance dataset with features \n",
    "\n",
    "Note that some datasets have more/less of a feature, e.g. more female speakers than male in shanghai dataset, so it gets rebalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ddf64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c2d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa03cc03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee280194",
   "metadata": {},
   "source": [
    "# Export data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321de572",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8784e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIX THIS \n",
    "# \n",
    "# # If your list is a list of tuples, specify column names:\n",
    "columns = [\"audio\", \"sampling_rate\", \"label\", \"text\", \"audio_length\", \"gender\", \"token\", \"sentiment\"]\n",
    "df = pd.DataFrame(combined_data_with_sentiment, columns=columns)\n",
    "\n",
    "df.to_csv(\"final_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
